// app/api/send-email/route.js
import { NextResponse } from "next/server";

export async function GET() {
  try {
    // Send email
    return NextResponse.json({
      id: "7a6c3afb-bbab-440d-9295-aba3c9d09150",
      data: {
        nodes: [
          {
            id: "ChatInput-6GHDU",
            type: "genericNode",
            position: { x: 1742.2390479909475, y: 649.320964448091 },
            data: {
              type: "ChatInput",
              node: {
                template: {
                  _type: "Component",
                  files: {
                    trace_as_metadata: true,
                    file_path: "",
                    fileTypes: [
                      "txt",
                      "md",
                      "mdx",
                      "csv",
                      "json",
                      "yaml",
                      "yml",
                      "xml",
                      "html",
                      "htm",
                      "pdf",
                      "docx",
                      "py",
                      "sh",
                      "sql",
                      "js",
                      "ts",
                      "tsx",
                      "jpg",
                      "jpeg",
                      "png",
                      "bmp",
                      "image",
                    ],
                    list: true,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "files",
                    value: "",
                    display_name: "Files",
                    advanced: true,
                    dynamic: false,
                    info: "Files to be sent with the message.",
                    title_case: false,
                    type: "file",
                    _input_type: "FileInput",
                  },
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = "Chat Input"\n    description = "Get chat inputs from the Playground."\n    icon = "ChatInput"\n    name = "ChatInput"\n\n    inputs = [\n        MultilineInput(\n            name="input_value",\n            display_name="Text",\n            value="",\n            info="Message to be passed as input.",\n        ),\n        BoolInput(\n            name="should_store_message",\n            display_name="Store Messages",\n            info="Store the message in the history.",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name="sender",\n            display_name="Sender Type",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info="Type of sender.",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name="sender_name",\n            display_name="Sender Name",\n            info="Name of the sender.",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name="session_id",\n            display_name="Session ID",\n            info="The session ID of the chat. If empty, the current session ID parameter will be used.",\n            advanced=True,\n        ),\n        FileInput(\n            name="files",\n            display_name="Files",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info="Files to be sent with the message.",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name="Message", name="message", method="message_response"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  input_value: {
                    trace_as_input: true,
                    multiline: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "input_value",
                    value: "im having reduced central vision",
                    display_name: "Text",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "Message to be passed as input.",
                    title_case: false,
                    type: "str",
                    _input_type: "MultilineInput",
                  },
                  sender: {
                    trace_as_metadata: true,
                    options: ["Machine", "User"],
                    combobox: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "sender",
                    value: "User",
                    display_name: "Sender Type",
                    advanced: true,
                    dynamic: false,
                    info: "Type of sender.",
                    title_case: false,
                    type: "str",
                    _input_type: "DropdownInput",
                  },
                  sender_name: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "sender_name",
                    value: "User",
                    display_name: "Sender Name",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "Name of the sender.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  session_id: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "session_id",
                    value: "",
                    display_name: "Session ID",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "The session ID of the chat. If empty, the current session ID parameter will be used.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  should_store_message: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "should_store_message",
                    value: true,
                    display_name: "Store Messages",
                    advanced: true,
                    dynamic: false,
                    info: "Store the message in the history.",
                    title_case: false,
                    type: "bool",
                    _input_type: "BoolInput",
                  },
                },
                description: "Get chat inputs from the Playground.",
                icon: "ChatInput",
                base_classes: ["Message"],
                display_name: "Chat Input",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Message"],
                    selected: "Message",
                    name: "message",
                    display_name: "Message",
                    method: "message_response",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                ],
                field_order: [
                  "input_value",
                  "should_store_message",
                  "sender",
                  "sender_name",
                  "session_id",
                  "files",
                ],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "ChatInput-6GHDU",
              description: "Get chat inputs from the Playground.",
              display_name: "Chat Input",
              showNode: true,
            },
            selected: false,
            width: 384,
            height: 288,
            dragging: false,
          },
          {
            id: "ChatOutput-4D7QB",
            type: "genericNode",
            position: { x: 4045.1769520755765, y: 783.4889851395819 },
            data: {
              type: "ChatOutput",
              node: {
                template: {
                  _type: "Component",
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = "Chat Output"\n    description = "Display a chat message in the Playground."\n    icon = "ChatOutput"\n    name = "ChatOutput"\n\n    inputs = [\n        MessageTextInput(\n            name="input_value",\n            display_name="Text",\n            info="Message to be passed as output.",\n        ),\n        BoolInput(\n            name="should_store_message",\n            display_name="Store Messages",\n            info="Store the message in the history.",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name="sender",\n            display_name="Sender Type",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info="Type of sender.",\n        ),\n        MessageTextInput(\n            name="sender_name",\n            display_name="Sender Name",\n            info="Name of the sender.",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name="session_id",\n            display_name="Session ID",\n            info="The session ID of the chat. If empty, the current session ID parameter will be used.",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name="data_template",\n            display_name="Data Template",\n            value="{text}",\n            advanced=True,\n            info="Template to convert Data to Text. If left empty, it will be dynamically set to the Data\'s text key.",\n        ),\n    ]\n    outputs = [\n        Output(display_name="Message", name="message", method="message_response"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  data_template: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "data_template",
                    value: "{text}",
                    display_name: "Data Template",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  input_value: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "input_value",
                    value: "",
                    display_name: "Text",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "Message to be passed as output.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  sender: {
                    trace_as_metadata: true,
                    options: ["Machine", "User"],
                    combobox: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "sender",
                    value: "Machine",
                    display_name: "Sender Type",
                    advanced: true,
                    dynamic: false,
                    info: "Type of sender.",
                    title_case: false,
                    type: "str",
                    _input_type: "DropdownInput",
                  },
                  sender_name: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "sender_name",
                    value: "Lei_AI",
                    display_name: "Sender Name",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "Name of the sender.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  session_id: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "session_id",
                    value: "",
                    display_name: "Session ID",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "The session ID of the chat. If empty, the current session ID parameter will be used.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  should_store_message: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "should_store_message",
                    value: true,
                    display_name: "Store Messages",
                    advanced: true,
                    dynamic: false,
                    info: "Store the message in the history.",
                    title_case: false,
                    type: "bool",
                    _input_type: "BoolInput",
                  },
                },
                description: "Display a chat message in the Playground.",
                icon: "ChatOutput",
                base_classes: ["Message"],
                display_name: "Chat Output",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Message"],
                    selected: "Message",
                    name: "message",
                    display_name: "Message",
                    method: "message_response",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                ],
                field_order: [
                  "input_value",
                  "should_store_message",
                  "sender",
                  "sender_name",
                  "session_id",
                  "data_template",
                ],
                beta: false,
                edited: false,
                lf_version: "1.0.19.post1",
              },
              id: "ChatOutput-4D7QB",
              showNode: true,
            },
            selected: false,
            width: 384,
            height: 288,
            dragging: false,
          },
          {
            id: "Prompt-1Fok7",
            type: "genericNode",
            position: { x: 2707.75398623635, y: 106.8200605295043 },
            data: {
              type: "Prompt",
              node: {
                template: {
                  _type: "Component",
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = "Prompt"\n    description: str = "Create a prompt template with dynamic variables."\n    icon = "prompts"\n    trace_type = "prompt"\n    name = "Prompt"\n\n    inputs = [\n        PromptInput(name="template", display_name="Template"),\n    ]\n\n    outputs = [\n        Output(display_name="Prompt Message", name="prompt", method="build_prompt"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node["template"]["template"]["value"]\n        custom_fields = frontend_node["custom_fields"]\n        frontend_node_template = frontend_node["template"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name="template",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        """\n        This function is called after the code validation is done.\n        """\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node["template"]["template"]["value"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name="template",\n            custom_fields=frontend_node["custom_fields"],\n            frontend_node_template=frontend_node["template"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node["template"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  template: {
                    trace_as_input: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "template",
                    value:
                      "Context: {context}\n\nYou are a helpful diagnostic assistant. Your role is to assist users with potential disease diagnoses and possible treatment plan based on the symptoms they describe. Be friendly and conversational in your responses.\n\nKeep responses concise and clear, focusing on the essential details.\n\nIf the user has not shared symptoms Politely prompt them to describe any symptoms they are experiencing.\n\nIf symptoms are shared and the symptoms matches multiple diseases, ask follow-up questions to narrow it down. Once the disease has been decided give a treatment plan for it\n\n\nMessage History: {history}\n\nQuestion: {question}",
                    display_name: "Template",
                    advanced: false,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "prompt",
                    _input_type: "PromptInput",
                  },
                  question: {
                    field_type: "str",
                    required: false,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value: "",
                    fileTypes: [],
                    file_path: "",
                    name: "question",
                    display_name: "question",
                    advanced: false,
                    input_types: ["Message", "Text"],
                    dynamic: false,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                    type: "str",
                  },
                  context: {
                    field_type: "str",
                    required: false,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value: "",
                    fileTypes: [],
                    file_path: "",
                    name: "context",
                    display_name: "context",
                    advanced: false,
                    input_types: ["Message", "Text"],
                    dynamic: false,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                    type: "str",
                  },
                  history: {
                    field_type: "str",
                    required: false,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value: "",
                    fileTypes: [],
                    file_path: "",
                    name: "history",
                    display_name: "history",
                    advanced: false,
                    input_types: ["Message", "Text"],
                    dynamic: false,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                    type: "str",
                  },
                },
                description: "Create a prompt template with dynamic variables.",
                icon: "prompts",
                is_input: null,
                is_output: null,
                is_composition: null,
                base_classes: ["Message"],
                name: "",
                display_name: "Prompt",
                documentation: "",
                custom_fields: { template: ["context", "history", "question"] },
                output_types: [],
                full_path: null,
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Message"],
                    selected: "Message",
                    name: "prompt",
                    hidden: null,
                    display_name: "Prompt Message",
                    method: "build_prompt",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: null,
                  },
                ],
                field_order: ["template"],
                beta: false,
                error: null,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "Prompt-1Fok7",
            },
            selected: false,
            width: 384,
            height: 561,
            positionAbsolute: { x: 2707.75398623635, y: 106.8200605295043 },
            dragging: false,
          },
          {
            id: "File-e5qwb",
            type: "genericNode",
            position: { x: 131.27064426833408, y: 1569.9185729009096 },
            data: {
              type: "File",
              node: {
                template: {
                  _type: "Component",
                  path: {
                    trace_as_metadata: true,
                    file_path:
                      "7a6c3afb-bbab-440d-9295-aba3c9d09150/2024-11-06_12-20-39_newintent.json",
                    fileTypes: [
                      "txt",
                      "md",
                      "mdx",
                      "csv",
                      "json",
                      "yaml",
                      "yml",
                      "xml",
                      "html",
                      "htm",
                      "pdf",
                      "docx",
                      "py",
                      "sh",
                      "sql",
                      "js",
                      "ts",
                      "tsx",
                    ],
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "path",
                    value: "",
                    display_name: "Path",
                    advanced: false,
                    dynamic: false,
                    info: "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx",
                    title_case: false,
                    type: "file",
                    _input_type: "FileInput",
                    load_from_db: false,
                  },
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from pathlib import Path\n\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, FileInput, Output\nfrom langflow.schema import Data\n\n\nclass FileComponent(Component):\n    display_name = "File"\n    description = "A generic file loader."\n    icon = "file-text"\n    name = "File"\n\n    inputs = [\n        FileInput(\n            name="path",\n            display_name="Path",\n            file_types=TEXT_FILE_TYPES,\n            info=f"Supported file types: {\', \'.join(TEXT_FILE_TYPES)}",\n        ),\n        BoolInput(\n            name="silent_errors",\n            display_name="Silent Errors",\n            advanced=True,\n            info="If true, errors will not raise an exception.",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name="Data", name="data", method="load_file"),\n    ]\n\n    def load_file(self) -> Data:\n        if not self.path:\n            msg = "Please, upload a file to use this component."\n            raise ValueError(msg)\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == "doc":\n            msg = "doc files are not supported. Please save as .docx"\n            raise ValueError(msg)\n        if extension not in TEXT_FILE_TYPES:\n            msg = f"Unsupported file type: {extension}"\n            raise ValueError(msg)\n\n        data = parse_text_file_to_data(resolved_path, silent_errors)\n        self.status = data or "No data"\n        return data or Data()\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  silent_errors: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "silent_errors",
                    value: false,
                    display_name: "Silent Errors",
                    advanced: true,
                    dynamic: false,
                    info: "If true, errors will not raise an exception.",
                    title_case: false,
                    type: "bool",
                    _input_type: "BoolInput",
                  },
                },
                description: "A generic file loader.",
                icon: "file-text",
                base_classes: ["Data"],
                display_name: "File",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Data"],
                    selected: "Data",
                    name: "data",
                    display_name: "Data",
                    method: "load_file",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                ],
                field_order: ["path", "silent_errors"],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "File-e5qwb",
              description: "A generic file loader.",
              display_name: "File",
              showNode: true,
            },
            selected: false,
            width: 384,
            height: 288,
            positionAbsolute: { x: 131.27064426833408, y: 1569.9185729009096 },
            dragging: false,
          },
          {
            id: "SplitText-0B9NJ",
            type: "genericNode",
            position: { x: 746.6416511227912, y: 1487.5528611492327 },
            data: {
              type: "SplitText",
              node: {
                template: {
                  _type: "Component",
                  data_inputs: {
                    trace_as_metadata: true,
                    list: true,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "data_inputs",
                    value: "",
                    display_name: "Data Inputs",
                    advanced: false,
                    input_types: ["Data"],
                    dynamic: false,
                    info: "The data to split.",
                    title_case: false,
                    type: "other",
                    _input_type: "HandleInput",
                  },
                  chunk_overlap: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chunk_overlap",
                    value: 200,
                    display_name: "Chunk Overlap",
                    advanced: false,
                    dynamic: false,
                    info: "Number of characters to overlap between chunks.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  chunk_size: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chunk_size",
                    value: 995,
                    display_name: "Chunk Size",
                    advanced: false,
                    dynamic: false,
                    info: "The maximum number of characters in each chunk.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                    load_from_db: false,
                  },
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from langchain_text_splitters import CharacterTextSplitter\n\nfrom langflow.custom import Component\nfrom langflow.io import HandleInput, IntInput, MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.utils.util import unescape_string\n\n\nclass SplitTextComponent(Component):\n    display_name: str = "Split Text"\n    description: str = "Split text into chunks based on specified criteria."\n    icon = "scissors-line-dashed"\n    name = "SplitText"\n\n    inputs = [\n        HandleInput(\n            name="data_inputs",\n            display_name="Data Inputs",\n            info="The data to split.",\n            input_types=["Data"],\n            is_list=True,\n        ),\n        IntInput(\n            name="chunk_overlap",\n            display_name="Chunk Overlap",\n            info="Number of characters to overlap between chunks.",\n            value=200,\n        ),\n        IntInput(\n            name="chunk_size",\n            display_name="Chunk Size",\n            info="The maximum number of characters in each chunk.",\n            value=1000,\n        ),\n        MessageTextInput(\n            name="separator",\n            display_name="Separator",\n            info="The character to split on. Defaults to newline.",\n            value="\\n",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name="Chunks", name="chunks", method="split_text"),\n    ]\n\n    def _docs_to_data(self, docs):\n        return [Data(text=doc.page_content, data=doc.metadata) for doc in docs]\n\n    def split_text(self) -> list[Data]:\n        separator = unescape_string(self.separator)\n\n        documents = [_input.to_lc_document() for _input in self.data_inputs if isinstance(_input, Data)]\n\n        splitter = CharacterTextSplitter(\n            chunk_overlap=self.chunk_overlap,\n            chunk_size=self.chunk_size,\n            separator=separator,\n        )\n        docs = splitter.split_documents(documents)\n        data = self._docs_to_data(docs)\n        self.status = data\n        return data\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  separator: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "separator",
                    value: "\n",
                    display_name: "Separator",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "The character to split on. Defaults to newline.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                },
                description:
                  "Split text into chunks based on specified criteria.",
                icon: "scissors-line-dashed",
                base_classes: ["Data"],
                display_name: "Split Text",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Data"],
                    selected: "Data",
                    name: "chunks",
                    display_name: "Chunks",
                    method: "split_text",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                ],
                field_order: [
                  "data_inputs",
                  "chunk_overlap",
                  "chunk_size",
                  "separator",
                ],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "SplitText-0B9NJ",
              description:
                "Split text into chunks based on specified criteria.",
              display_name: "Split Text",
            },
            selected: false,
            width: 384,
            height: 507,
            dragging: false,
            positionAbsolute: { x: 746.6416511227912, y: 1487.5528611492327 },
          },
          {
            id: "Chroma-dOmCo",
            type: "genericNode",
            position: { x: 2248.0401519367374, y: 1417.262769984066 },
            data: {
              type: "Chroma",
              node: {
                template: {
                  _type: "Component",
                  embedding: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "embedding",
                    value: "",
                    display_name: "Embedding",
                    advanced: false,
                    input_types: ["Embeddings"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "other",
                    _input_type: "HandleInput",
                  },
                  ingest_data: {
                    trace_as_metadata: true,
                    list: true,
                    trace_as_input: true,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "ingest_data",
                    value: "",
                    display_name: "Ingest Data",
                    advanced: false,
                    input_types: ["Data"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "other",
                    _input_type: "DataInput",
                  },
                  allow_duplicates: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "allow_duplicates",
                    value: false,
                    display_name: "Allow Duplicates",
                    advanced: true,
                    dynamic: false,
                    info: "If false, will not add documents that are already in the Vector Store.",
                    title_case: false,
                    type: "bool",
                    _input_type: "BoolInput",
                  },
                  chroma_server_cors_allow_origins: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_cors_allow_origins",
                    value: "",
                    display_name: "Server CORS Allow Origins",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  chroma_server_grpc_port: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_grpc_port",
                    value: "",
                    display_name: "Server gRPC Port",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  chroma_server_host: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_host",
                    value: "",
                    display_name: "Server Host",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  chroma_server_http_port: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_http_port",
                    value: "",
                    display_name: "Server HTTP Port",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  chroma_server_ssl_enabled: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_ssl_enabled",
                    value: false,
                    display_name: "Server SSL Enabled",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "bool",
                    _input_type: "BoolInput",
                  },
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from copy import deepcopy\n\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.io import BoolInput, DataInput, DropdownInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    """\n    Chroma Vector Store with search capabilities\n    """\n\n    display_name: str = "Chroma DB"\n    description: str = "Chroma Vector Store with search capabilities"\n    documentation = "https://python.langchain.com/docs/integrations/vectorstores/chroma"\n    name = "Chroma"\n    icon = "Chroma"\n\n    inputs = [\n        StrInput(\n            name="collection_name",\n            display_name="Collection Name",\n            value="langflow",\n        ),\n        StrInput(\n            name="persist_directory",\n            display_name="Persist Directory",\n        ),\n        MultilineInput(\n            name="search_query",\n            display_name="Search Query",\n        ),\n        DataInput(\n            name="ingest_data",\n            display_name="Ingest Data",\n            is_list=True,\n        ),\n        HandleInput(name="embedding", display_name="Embedding", input_types=["Embeddings"]),\n        StrInput(\n            name="chroma_server_cors_allow_origins",\n            display_name="Server CORS Allow Origins",\n            advanced=True,\n        ),\n        StrInput(\n            name="chroma_server_host",\n            display_name="Server Host",\n            advanced=True,\n        ),\n        IntInput(\n            name="chroma_server_http_port",\n            display_name="Server HTTP Port",\n            advanced=True,\n        ),\n        IntInput(\n            name="chroma_server_grpc_port",\n            display_name="Server gRPC Port",\n            advanced=True,\n        ),\n        BoolInput(\n            name="chroma_server_ssl_enabled",\n            display_name="Server SSL Enabled",\n            advanced=True,\n        ),\n        BoolInput(\n            name="allow_duplicates",\n            display_name="Allow Duplicates",\n            advanced=True,\n            info="If false, will not add documents that are already in the Vector Store.",\n        ),\n        DropdownInput(\n            name="search_type",\n            display_name="Search Type",\n            options=["Similarity", "MMR"],\n            value="Similarity",\n            advanced=True,\n        ),\n        IntInput(\n            name="number_of_results",\n            display_name="Number of Results",\n            info="Number of results to return.",\n            advanced=True,\n            value=10,\n        ),\n        IntInput(\n            name="limit",\n            display_name="Limit",\n            advanced=True,\n            info="Limit the number of records to compare when Allow Duplicates is False.",\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> Chroma:\n        """\n        Builds the Chroma object.\n        """\n        try:\n            from chromadb import Client\n            from langchain_chroma import Chroma\n        except ImportError as e:\n            msg = "Could not import Chroma integration package. Please install it with `pip install langchain-chroma`."\n            raise ImportError(msg) from e\n        # Chroma settings\n        chroma_settings = None\n        client = None\n        if self.chroma_server_host:\n            chroma_settings = Settings(\n                chroma_server_cors_allow_origins=self.chroma_server_cors_allow_origins or [],\n                chroma_server_host=self.chroma_server_host,\n                chroma_server_http_port=self.chroma_server_http_port or None,\n                chroma_server_grpc_port=self.chroma_server_grpc_port or None,\n                chroma_server_ssl_enabled=self.chroma_server_ssl_enabled,\n            )\n            client = Client(settings=chroma_settings)\n\n        # Check persist_directory and expand it if it is a relative path\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory is not None else None\n\n        chroma = Chroma(\n            persist_directory=persist_directory,\n            client=client,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n\n        self._add_documents_to_vector_store(chroma)\n        self.status = chroma_collection_to_data(chroma.get(limit=self.limit))\n        return chroma\n\n    def _add_documents_to_vector_store(self, vector_store: "Chroma") -> None:\n        """\n        Adds documents to the Vector Store.\n        """\n        if not self.ingest_data:\n            self.status = ""\n            return\n\n        _stored_documents_without_id = []\n        if self.allow_duplicates:\n            stored_data = []\n        else:\n            stored_data = chroma_collection_to_data(vector_store.get(limit=self.limit))\n            for value in deepcopy(stored_data):\n                del value.id\n                _stored_documents_without_id.append(value)\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                if _input not in _stored_documents_without_id:\n                    documents.append(_input.to_lc_document())\n            else:\n                msg = "Vector Store Inputs must be Data objects."\n                raise TypeError(msg)\n\n        if documents and self.embedding is not None:\n            logger.debug(f"Adding {len(documents)} documents to the Vector Store.")\n            vector_store.add_documents(documents)\n        else:\n            logger.debug("No documents to add to the Vector Store.")\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  collection_name: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "collection_name",
                    value: "newlangflow2",
                    display_name: "Collection Name",
                    advanced: false,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  limit: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "limit",
                    value: "",
                    display_name: "Limit",
                    advanced: true,
                    dynamic: false,
                    info: "Limit the number of records to compare when Allow Duplicates is False.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  number_of_results: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "number_of_results",
                    value: 10,
                    display_name: "Number of Results",
                    advanced: true,
                    dynamic: false,
                    info: "Number of results to return.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  persist_directory: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "persist_directory",
                    value: "",
                    display_name: "Persist Directory",
                    advanced: false,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  search_query: {
                    trace_as_input: true,
                    multiline: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "search_query",
                    value: "",
                    display_name: "Search Query",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "MultilineInput",
                  },
                  search_type: {
                    trace_as_metadata: true,
                    options: ["Similarity", "MMR"],
                    combobox: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "search_type",
                    value: "Similarity",
                    display_name: "Search Type",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "DropdownInput",
                  },
                },
                description: "Chroma Vector Store with search capabilities",
                icon: "Chroma",
                base_classes: ["Data", "Retriever", "VectorStore"],
                display_name: "Chroma DB",
                documentation:
                  "https://python.langchain.com/docs/integrations/vectorstores/chroma",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Retriever"],
                    selected: "Retriever",
                    name: "base_retriever",
                    display_name: "Retriever",
                    method: "build_base_retriever",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: [],
                  },
                  {
                    types: ["Data"],
                    selected: "Data",
                    name: "search_results",
                    display_name: "Search Results",
                    method: "search_documents",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: [
                      "number_of_results",
                      "search_query",
                      "search_type",
                    ],
                  },
                  {
                    types: ["VectorStore"],
                    selected: "VectorStore",
                    name: "vector_store",
                    display_name: "Vector Store",
                    method: "cast_vector_store",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: [],
                  },
                ],
                field_order: [
                  "collection_name",
                  "persist_directory",
                  "search_query",
                  "ingest_data",
                  "embedding",
                  "chroma_server_cors_allow_origins",
                  "chroma_server_host",
                  "chroma_server_http_port",
                  "chroma_server_grpc_port",
                  "chroma_server_ssl_enabled",
                  "allow_duplicates",
                  "search_type",
                  "number_of_results",
                  "limit",
                ],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "Chroma-dOmCo",
              description: "Chroma Vector Store with search capabilities",
              display_name: "Chroma DB",
            },
            selected: false,
            width: 384,
            height: 635,
          },
          {
            id: "ParseData-GuEEQ",
            type: "genericNode",
            position: { x: 2797.188289745623, y: 1003.8399800260631 },
            data: {
              type: "ParseData",
              node: {
                template: {
                  _type: "Component",
                  data: {
                    trace_as_metadata: true,
                    list: false,
                    trace_as_input: true,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "data",
                    value: "",
                    display_name: "Data",
                    advanced: false,
                    input_types: ["Data"],
                    dynamic: false,
                    info: "The data to convert to text.",
                    title_case: false,
                    type: "other",
                    _input_type: "DataInput",
                  },
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = "Parse Data"\n    description = "Convert Data into plain text following a specified template."\n    icon = "braces"\n    name = "ParseData"\n\n    inputs = [\n        DataInput(name="data", display_name="Data", info="The data to convert to text."),\n        MultilineInput(\n            name="template",\n            display_name="Template",\n            info="The template to use for formatting the data. "\n            "It can contain the keys {text}, {data} or any other key in the Data.",\n            value="{text}",\n        ),\n        StrInput(name="sep", display_name="Separator", advanced=True, value="\\n"),\n    ]\n\n    outputs = [\n        Output(display_name="Text", name="text", method="parse_data"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  sep: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "sep",
                    value: "\n",
                    display_name: "Separator",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  template: {
                    trace_as_input: true,
                    multiline: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "template",
                    value: "{text}",
                    display_name: "Template",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                    title_case: false,
                    type: "str",
                    _input_type: "MultilineInput",
                  },
                },
                description:
                  "Convert Data into plain text following a specified template.",
                icon: "braces",
                base_classes: ["Message"],
                display_name: "Parse Data",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Message"],
                    selected: "Message",
                    name: "text",
                    display_name: "Text",
                    method: "parse_data",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                ],
                field_order: ["data", "template", "sep"],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "ParseData-GuEEQ",
              description:
                "Convert Data into plain text following a specified template.",
              display_name: "Parse Data",
            },
            selected: false,
            width: 384,
            height: 352,
            positionAbsolute: { x: 2797.188289745623, y: 1003.8399800260631 },
            dragging: false,
          },
          {
            id: "Memory-aCsXk",
            type: "genericNode",
            position: { x: 2096.283108114828, y: -2.649993896484375 },
            data: {
              type: "Memory",
              node: {
                template: {
                  _type: "Component",
                  memory: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "memory",
                    value: "",
                    display_name: "External Memory",
                    advanced: false,
                    input_types: ["BaseChatMessageHistory"],
                    dynamic: false,
                    info: "Retrieve messages from an external memory. If empty, it will use the Langflow tables.",
                    title_case: false,
                    type: "other",
                    _input_type: "HandleInput",
                  },
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from langchain.memory import ConversationBufferMemory\n\nfrom langflow.custom import Component\nfrom langflow.field_typing import BaseChatMemory\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import LCBuiltinChatMemory, get_messages\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\n\nclass MemoryComponent(Component):\n    display_name = "Chat Memory"\n    description = "Retrieves stored chat messages from Langflow tables or an external memory."\n    icon = "message-square-more"\n    name = "Memory"\n\n    inputs = [\n        HandleInput(\n            name="memory",\n            display_name="External Memory",\n            input_types=["BaseChatMessageHistory"],\n            info="Retrieve messages from an external memory. If empty, it will use the Langflow tables.",\n        ),\n        DropdownInput(\n            name="sender",\n            display_name="Sender Type",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, "Machine and User"],\n            value="Machine and User",\n            info="Filter by sender type.",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name="sender_name",\n            display_name="Sender Name",\n            info="Filter by sender name.",\n            advanced=True,\n        ),\n        IntInput(\n            name="n_messages",\n            display_name="Number of Messages",\n            value=100,\n            info="Number of messages to retrieve.",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name="session_id",\n            display_name="Session ID",\n            info="The session ID of the chat. If empty, the current session ID parameter will be used.",\n            advanced=True,\n        ),\n        DropdownInput(\n            name="order",\n            display_name="Order",\n            options=["Ascending", "Descending"],\n            value="Ascending",\n            info="Order of the messages.",\n            advanced=True,\n        ),\n        MultilineInput(\n            name="template",\n            display_name="Template",\n            info="The template to use for formatting the data. "\n            "It can contain the keys {text}, {sender} or any other key in the message data.",\n            value="{sender_name}: {text}",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name="Messages (Data)", name="messages", method="retrieve_messages"),\n        Output(display_name="Messages (Text)", name="messages_text", method="retrieve_messages_as_text"),\n        Output(display_name="Memory", name="lc_memory", method="build_lc_memory"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = "DESC" if self.order == "Descending" else "ASC"\n\n        if sender == "Machine and User":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            # langchain memories are supposed to return messages in ascending order\n            if order == "DESC":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n            if sender:\n                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER\n                stored = [m for m in stored if m.type == expected_type]\n        else:\n            stored = get_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        chat_memory = self.memory or LCBuiltinChatMemory(flow_id=self.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  n_messages: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "n_messages",
                    value: 100,
                    display_name: "Number of Messages",
                    advanced: true,
                    dynamic: false,
                    info: "Number of messages to retrieve.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  order: {
                    trace_as_metadata: true,
                    options: ["Ascending", "Descending"],
                    combobox: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "order",
                    value: "Ascending",
                    display_name: "Order",
                    advanced: true,
                    dynamic: false,
                    info: "Order of the messages.",
                    title_case: false,
                    type: "str",
                    _input_type: "DropdownInput",
                  },
                  sender: {
                    trace_as_metadata: true,
                    options: ["Machine", "User", "Machine and User"],
                    combobox: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "sender",
                    value: "Machine and User",
                    display_name: "Sender Type",
                    advanced: true,
                    dynamic: false,
                    info: "Filter by sender type.",
                    title_case: false,
                    type: "str",
                    _input_type: "DropdownInput",
                  },
                  sender_name: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "sender_name",
                    value: "",
                    display_name: "Sender Name",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "Filter by sender name.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  session_id: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "session_id",
                    value: "",
                    display_name: "Session ID",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "The session ID of the chat. If empty, the current session ID parameter will be used.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  template: {
                    trace_as_input: true,
                    multiline: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "template",
                    value: "{sender_name}: {text}",
                    display_name: "Template",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.",
                    title_case: false,
                    type: "str",
                    _input_type: "MultilineInput",
                  },
                },
                description:
                  "Retrieves stored chat messages from Langflow tables or an external memory.",
                icon: "message-square-more",
                base_classes: ["BaseChatMemory", "Data", "Message"],
                display_name: "Chat Memory",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Data"],
                    selected: "Data",
                    name: "messages",
                    display_name: "Messages (Data)",
                    method: "retrieve_messages",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                  {
                    types: ["Message"],
                    selected: "Message",
                    name: "messages_text",
                    display_name: "Messages (Text)",
                    method: "retrieve_messages_as_text",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                  {
                    types: ["BaseChatMemory"],
                    selected: "BaseChatMemory",
                    name: "lc_memory",
                    display_name: "Memory",
                    method: "build_lc_memory",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                ],
                field_order: [
                  "memory",
                  "sender",
                  "sender_name",
                  "n_messages",
                  "session_id",
                  "order",
                  "template",
                ],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "Memory-aCsXk",
              description:
                "Retrieves stored chat messages from Langflow tables or an external memory.",
              display_name: "Chat Memory",
            },
            selected: false,
            width: 384,
            height: 346,
            dragging: false,
          },
          {
            id: "TextInput-eczLx",
            type: "genericNode",
            position: { x: 1047.0274740035295, y: 230.005040443114 },
            data: {
              type: "TextInput",
              node: {
                template: {
                  _type: "Component",
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = "Text Input"\n    description = "Get text inputs from the Playground."\n    icon = "type"\n    name = "TextInput"\n\n    inputs = [\n        MultilineInput(\n            name="input_value",\n            display_name="Text",\n            info="Text to be passed as input.",\n        ),\n    ]\n    outputs = [\n        Output(display_name="Text", name="text", method="text_response"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  input_value: {
                    trace_as_input: true,
                    multiline: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "input_value",
                    value: "",
                    display_name: "Text",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "Text to be passed as input.",
                    title_case: false,
                    type: "str",
                    _input_type: "MultilineInput",
                  },
                },
                description: "Get text inputs from the Playground.",
                icon: "type",
                base_classes: ["Message"],
                display_name: "Name",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Message"],
                    selected: "Message",
                    name: "text",
                    display_name: "Text",
                    method: "text_response",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                ],
                field_order: ["input_value"],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "TextInput-eczLx",
              description: "Get text inputs from the Playground.",
              display_name: "Name",
              showNode: true,
            },
            selected: false,
            width: 384,
            height: 288,
            dragging: false,
          },
          {
            id: "MistalAIEmbeddings-KEi53",
            type: "genericNode",
            position: { x: 991.9744218149965, y: 2302.556560796782 },
            data: {
              type: "MistalAIEmbeddings",
              node: {
                template: {
                  _type: "Component",
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from langchain_mistralai.embeddings import MistralAIEmbeddings\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\n\n\nclass MistralAIEmbeddingsComponent(LCModelComponent):\n    display_name = "MistralAI Embeddings"\n    description = "Generate embeddings using MistralAI models."\n    icon = "MistralAI"\n    name = "MistalAIEmbeddings"\n\n    inputs = [\n        DropdownInput(\n            name="model",\n            display_name="Model",\n            advanced=False,\n            options=["mistral-embed"],\n            value="mistral-embed",\n        ),\n        SecretStrInput(name="mistral_api_key", display_name="Mistral API Key"),\n        IntInput(\n            name="max_concurrent_requests",\n            display_name="Max Concurrent Requests",\n            advanced=True,\n            value=64,\n        ),\n        IntInput(name="max_retries", display_name="Max Retries", advanced=True, value=5),\n        IntInput(name="timeout", display_name="Request Timeout", advanced=True, value=120),\n        MessageTextInput(\n            name="endpoint",\n            display_name="API Endpoint",\n            advanced=True,\n            value="https://api.mistral.ai/v1/",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name="Embeddings", name="embeddings", method="build_embeddings"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        if not self.mistral_api_key:\n            msg = "Mistral API Key is required"\n            raise ValueError(msg)\n\n        api_key = SecretStr(self.mistral_api_key)\n\n        return MistralAIEmbeddings(\n            api_key=api_key,\n            model=self.model,\n            endpoint=self.endpoint,\n            max_concurrent_requests=self.max_concurrent_requests,\n            max_retries=self.max_retries,\n            timeout=self.timeout,\n        )\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  endpoint: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "endpoint",
                    value: "https://api.mistral.ai/v1/",
                    display_name: "API Endpoint",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  max_concurrent_requests: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "max_concurrent_requests",
                    value: 64,
                    display_name: "Max Concurrent Requests",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  max_retries: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "max_retries",
                    value: 5,
                    display_name: "Max Retries",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  mistral_api_key: {
                    load_from_db: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "mistral_api_key",
                    value: "",
                    display_name: "Mistral API Key",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    password: true,
                    type: "str",
                    _input_type: "SecretStrInput",
                  },
                  model: {
                    trace_as_metadata: true,
                    options: ["mistral-embed"],
                    combobox: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "model",
                    value: "mistral-embed",
                    display_name: "Model",
                    advanced: false,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "DropdownInput",
                  },
                  timeout: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "timeout",
                    value: 120,
                    display_name: "Request Timeout",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                },
                description: "Generate embeddings using MistralAI models.",
                icon: "MistralAI",
                base_classes: ["Embeddings"],
                display_name: "MistralAI Embeddings",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Embeddings"],
                    selected: "Embeddings",
                    name: "embeddings",
                    display_name: "Embeddings",
                    method: "build_embeddings",
                    value: "__UNDEFINED__",
                    cache: true,
                  },
                ],
                field_order: [
                  "model",
                  "mistral_api_key",
                  "max_concurrent_requests",
                  "max_retries",
                  "timeout",
                  "endpoint",
                ],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "MistalAIEmbeddings-KEi53",
              description: "Generate embeddings using MistralAI models.",
              display_name: "MistralAI Embeddings",
              showNode: false,
            },
            selected: false,
            width: 96,
            height: 96,
            dragging: false,
            positionAbsolute: { x: 991.9744218149965, y: 2302.556560796782 },
          },
          {
            id: "Chroma-gXf1F",
            type: "genericNode",
            position: { x: 1531.1680981351521, y: 1413.095495105653 },
            data: {
              type: "Chroma",
              node: {
                template: {
                  _type: "Component",
                  embedding: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "embedding",
                    value: "",
                    display_name: "Embedding",
                    advanced: false,
                    input_types: ["Embeddings"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "other",
                    _input_type: "HandleInput",
                  },
                  ingest_data: {
                    trace_as_metadata: true,
                    list: true,
                    trace_as_input: true,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "ingest_data",
                    value: "",
                    display_name: "Ingest Data",
                    advanced: false,
                    input_types: ["Data"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "other",
                    _input_type: "DataInput",
                  },
                  allow_duplicates: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "allow_duplicates",
                    value: false,
                    display_name: "Allow Duplicates",
                    advanced: true,
                    dynamic: false,
                    info: "If false, will not add documents that are already in the Vector Store.",
                    title_case: false,
                    type: "bool",
                    _input_type: "BoolInput",
                  },
                  chroma_server_cors_allow_origins: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_cors_allow_origins",
                    value: "",
                    display_name: "Server CORS Allow Origins",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  chroma_server_grpc_port: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_grpc_port",
                    value: "",
                    display_name: "Server gRPC Port",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  chroma_server_host: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_host",
                    value: "",
                    display_name: "Server Host",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  chroma_server_http_port: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_http_port",
                    value: "",
                    display_name: "Server HTTP Port",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  chroma_server_ssl_enabled: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "chroma_server_ssl_enabled",
                    value: false,
                    display_name: "Server SSL Enabled",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "bool",
                    _input_type: "BoolInput",
                  },
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'from copy import deepcopy\n\nfrom chromadb.config import Settings\nfrom langchain_chroma import Chroma\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.utils import chroma_collection_to_data\nfrom langflow.io import BoolInput, DataInput, DropdownInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass ChromaVectorStoreComponent(LCVectorStoreComponent):\n    """\n    Chroma Vector Store with search capabilities\n    """\n\n    display_name: str = "Chroma DB"\n    description: str = "Chroma Vector Store with search capabilities"\n    documentation = "https://python.langchain.com/docs/integrations/vectorstores/chroma"\n    name = "Chroma"\n    icon = "Chroma"\n\n    inputs = [\n        StrInput(\n            name="collection_name",\n            display_name="Collection Name",\n            value="langflow",\n        ),\n        StrInput(\n            name="persist_directory",\n            display_name="Persist Directory",\n        ),\n        MultilineInput(\n            name="search_query",\n            display_name="Search Query",\n        ),\n        DataInput(\n            name="ingest_data",\n            display_name="Ingest Data",\n            is_list=True,\n        ),\n        HandleInput(name="embedding", display_name="Embedding", input_types=["Embeddings"]),\n        StrInput(\n            name="chroma_server_cors_allow_origins",\n            display_name="Server CORS Allow Origins",\n            advanced=True,\n        ),\n        StrInput(\n            name="chroma_server_host",\n            display_name="Server Host",\n            advanced=True,\n        ),\n        IntInput(\n            name="chroma_server_http_port",\n            display_name="Server HTTP Port",\n            advanced=True,\n        ),\n        IntInput(\n            name="chroma_server_grpc_port",\n            display_name="Server gRPC Port",\n            advanced=True,\n        ),\n        BoolInput(\n            name="chroma_server_ssl_enabled",\n            display_name="Server SSL Enabled",\n            advanced=True,\n        ),\n        BoolInput(\n            name="allow_duplicates",\n            display_name="Allow Duplicates",\n            advanced=True,\n            info="If false, will not add documents that are already in the Vector Store.",\n        ),\n        DropdownInput(\n            name="search_type",\n            display_name="Search Type",\n            options=["Similarity", "MMR"],\n            value="Similarity",\n            advanced=True,\n        ),\n        IntInput(\n            name="number_of_results",\n            display_name="Number of Results",\n            info="Number of results to return.",\n            advanced=True,\n            value=10,\n        ),\n        IntInput(\n            name="limit",\n            display_name="Limit",\n            advanced=True,\n            info="Limit the number of records to compare when Allow Duplicates is False.",\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> Chroma:\n        """\n        Builds the Chroma object.\n        """\n        try:\n            from chromadb import Client\n            from langchain_chroma import Chroma\n        except ImportError as e:\n            msg = "Could not import Chroma integration package. Please install it with `pip install langchain-chroma`."\n            raise ImportError(msg) from e\n        # Chroma settings\n        chroma_settings = None\n        client = None\n        if self.chroma_server_host:\n            chroma_settings = Settings(\n                chroma_server_cors_allow_origins=self.chroma_server_cors_allow_origins or [],\n                chroma_server_host=self.chroma_server_host,\n                chroma_server_http_port=self.chroma_server_http_port or None,\n                chroma_server_grpc_port=self.chroma_server_grpc_port or None,\n                chroma_server_ssl_enabled=self.chroma_server_ssl_enabled,\n            )\n            client = Client(settings=chroma_settings)\n\n        # Check persist_directory and expand it if it is a relative path\n        persist_directory = self.resolve_path(self.persist_directory) if self.persist_directory is not None else None\n\n        chroma = Chroma(\n            persist_directory=persist_directory,\n            client=client,\n            embedding_function=self.embedding,\n            collection_name=self.collection_name,\n        )\n\n        self._add_documents_to_vector_store(chroma)\n        self.status = chroma_collection_to_data(chroma.get(limit=self.limit))\n        return chroma\n\n    def _add_documents_to_vector_store(self, vector_store: "Chroma") -> None:\n        """\n        Adds documents to the Vector Store.\n        """\n        if not self.ingest_data:\n            self.status = ""\n            return\n\n        _stored_documents_without_id = []\n        if self.allow_duplicates:\n            stored_data = []\n        else:\n            stored_data = chroma_collection_to_data(vector_store.get(limit=self.limit))\n            for value in deepcopy(stored_data):\n                del value.id\n                _stored_documents_without_id.append(value)\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                if _input not in _stored_documents_without_id:\n                    documents.append(_input.to_lc_document())\n            else:\n                msg = "Vector Store Inputs must be Data objects."\n                raise TypeError(msg)\n\n        if documents and self.embedding is not None:\n            logger.debug(f"Adding {len(documents)} documents to the Vector Store.")\n            vector_store.add_documents(documents)\n        else:\n            logger.debug("No documents to add to the Vector Store.")\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  collection_name: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "collection_name",
                    value: "newlangflow2",
                    display_name: "Collection Name",
                    advanced: false,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  limit: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "limit",
                    value: "",
                    display_name: "Limit",
                    advanced: true,
                    dynamic: false,
                    info: "Limit the number of records to compare when Allow Duplicates is False.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  number_of_results: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "number_of_results",
                    value: 10,
                    display_name: "Number of Results",
                    advanced: true,
                    dynamic: false,
                    info: "Number of results to return.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  persist_directory: {
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "persist_directory",
                    value: "",
                    display_name: "Persist Directory",
                    advanced: false,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "StrInput",
                  },
                  search_query: {
                    trace_as_input: true,
                    multiline: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "search_query",
                    value: "",
                    display_name: "Search Query",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "MultilineInput",
                  },
                  search_type: {
                    trace_as_metadata: true,
                    options: ["Similarity", "MMR"],
                    combobox: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "search_type",
                    value: "Similarity",
                    display_name: "Search Type",
                    advanced: true,
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "DropdownInput",
                  },
                },
                description: "Chroma Vector Store with search capabilities",
                icon: "Chroma",
                base_classes: ["Data", "Retriever", "VectorStore"],
                display_name: "Chroma DB",
                documentation:
                  "https://python.langchain.com/docs/integrations/vectorstores/chroma",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Retriever"],
                    selected: "Retriever",
                    name: "base_retriever",
                    display_name: "Retriever",
                    method: "build_base_retriever",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: [],
                  },
                  {
                    types: ["Data"],
                    selected: "Data",
                    name: "search_results",
                    display_name: "Search Results",
                    method: "search_documents",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: [
                      "number_of_results",
                      "search_query",
                      "search_type",
                    ],
                  },
                  {
                    types: ["VectorStore"],
                    selected: "VectorStore",
                    name: "vector_store",
                    display_name: "Vector Store",
                    method: "cast_vector_store",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: [],
                  },
                ],
                field_order: [
                  "collection_name",
                  "persist_directory",
                  "search_query",
                  "ingest_data",
                  "embedding",
                  "chroma_server_cors_allow_origins",
                  "chroma_server_host",
                  "chroma_server_http_port",
                  "chroma_server_grpc_port",
                  "chroma_server_ssl_enabled",
                  "allow_duplicates",
                  "search_type",
                  "number_of_results",
                  "limit",
                ],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "Chroma-gXf1F",
              description: "Chroma Vector Store with search capabilities",
              display_name: "Chroma DB",
            },
            selected: true,
            width: 384,
            height: 635,
            dragging: false,
          },
          {
            id: "GroqModel-HCKnM",
            type: "genericNode",
            position: { x: 3404.128301641932, y: 188.7753644352846 },
            data: {
              type: "GroqModel",
              node: {
                template: {
                  _type: "Component",
                  output_parser: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "output_parser",
                    value: "",
                    display_name: "Output Parser",
                    advanced: true,
                    input_types: ["OutputParser"],
                    dynamic: false,
                    info: "The parser to use to parse the output of the model",
                    title_case: false,
                    type: "other",
                    _input_type: "HandleInput",
                  },
                  code: {
                    type: "code",
                    required: true,
                    placeholder: "",
                    list: false,
                    show: true,
                    multiline: true,
                    value:
                      'import requests\nfrom langchain_groq import ChatGroq\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass GroqModel(LCModelComponent):\n    display_name: str = "Groq"\n    description: str = "Generate text using Groq."\n    icon = "Groq"\n    name = "GroqModel"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        SecretStrInput(name="groq_api_key", display_name="Groq API Key", info="API key for the Groq API."),\n        MessageTextInput(\n            name="groq_api_base",\n            display_name="Groq API Base",\n            info="Base URL path for API requests, leave blank if not using a proxy or service emulator.",\n            advanced=True,\n            value="https://api.groq.com",\n        ),\n        IntInput(\n            name="max_tokens",\n            display_name="Max Output Tokens",\n            info="The maximum number of tokens to generate.",\n            advanced=True,\n        ),\n        FloatInput(\n            name="temperature",\n            display_name="Temperature",\n            info="Run inference with this temperature. Must by in the closed interval [0.0, 1.0].",\n            value=0.1,\n        ),\n        IntInput(\n            name="n",\n            display_name="N",\n            info="Number of chat completions to generate for each prompt. "\n            "Note that the API may not return the full n completions if duplicates are generated.",\n            advanced=True,\n        ),\n        DropdownInput(\n            name="model_name",\n            display_name="Model",\n            info="The name of the model to use.",\n            options=[],\n            refresh_button=True,\n        ),\n        HandleInput(\n            name="output_parser",\n            display_name="Output Parser",\n            info="The parser to use to parse the output of the model",\n            advanced=True,\n            input_types=["OutputParser"],\n        ),\n    ]\n\n    def get_models(self) -> list[str]:\n        api_key = self.groq_api_key\n        base_url = self.groq_api_base or "https://api.groq.com"\n        url = f"{base_url}/openai/v1/models"\n\n        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}\n\n        try:\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            model_list = response.json()\n            return [model["id"] for model in model_list.get("data", [])]\n        except requests.RequestException as e:\n            self.status = f"Error fetching models: {e}"\n            return []\n\n    def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None):\n        if field_name in ("groq_api_key", "groq_api_base", "model_name"):\n            models = self.get_models()\n            build_config["model_name"]["options"] = models\n        return build_config\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        groq_api_key = self.groq_api_key\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        temperature = self.temperature\n        groq_api_base = self.groq_api_base\n        n = self.n\n        stream = self.stream\n\n        return ChatGroq(\n            model=model_name,\n            max_tokens=max_tokens or None,\n            temperature=temperature,\n            base_url=groq_api_base,\n            n=n or 1,\n            api_key=SecretStr(groq_api_key),\n            streaming=stream,\n        )\n',
                    fileTypes: [],
                    file_path: "",
                    password: false,
                    name: "code",
                    advanced: true,
                    dynamic: true,
                    info: "",
                    load_from_db: false,
                    title_case: false,
                  },
                  groq_api_base: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "groq_api_base",
                    value: "https://api.groq.com",
                    display_name: "Groq API Base",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "Base URL path for API requests, leave blank if not using a proxy or service emulator.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  groq_api_key: {
                    load_from_db: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "groq_api_key",
                    value: "",
                    display_name: "Groq API Key",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "API key for the Groq API.",
                    title_case: false,
                    password: true,
                    type: "str",
                    _input_type: "SecretStrInput",
                  },
                  input_value: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "input_value",
                    value: "",
                    display_name: "Input",
                    advanced: false,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageInput",
                  },
                  max_tokens: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "max_tokens",
                    value: "",
                    display_name: "Max Output Tokens",
                    advanced: true,
                    dynamic: false,
                    info: "The maximum number of tokens to generate.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  model_name: {
                    trace_as_metadata: true,
                    options: [
                      "llama3-groq-8b-8192-tool-use-preview",
                      "llama-3.2-90b-text-preview",
                      "llama-3.1-70b-versatile",
                      "llama-3.1-8b-instant",
                      "whisper-large-v3-turbo",
                      "llama-3.2-90b-vision-preview",
                      "llama3-8b-8192",
                      "llama-guard-3-8b",
                      "llama3-groq-70b-8192-tool-use-preview",
                      "llama-3.2-11b-text-preview",
                      "llama-3.2-3b-preview",
                      "llama-3.2-1b-preview",
                      "gemma-7b-it",
                      "whisper-large-v3",
                      "llama-3.2-11b-vision-preview",
                      "gemma2-9b-it",
                      "llama3-70b-8192",
                      "distil-whisper-large-v3-en",
                      "llava-v1.5-7b-4096-preview",
                      "mixtral-8x7b-32768",
                    ],
                    combobox: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "model_name",
                    value: "mixtral-8x7b-32768",
                    display_name: "Model",
                    advanced: false,
                    dynamic: false,
                    info: "The name of the model to use.",
                    refresh_button: true,
                    title_case: false,
                    type: "str",
                    _input_type: "DropdownInput",
                    load_from_db: false,
                  },
                  n: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "n",
                    value: "",
                    display_name: "N",
                    advanced: true,
                    dynamic: false,
                    info: "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                    title_case: false,
                    type: "int",
                    _input_type: "IntInput",
                  },
                  stream: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "stream",
                    value: false,
                    display_name: "Stream",
                    advanced: true,
                    dynamic: false,
                    info: "Stream the response from the model. Streaming works only in Chat.",
                    title_case: false,
                    type: "bool",
                    _input_type: "BoolInput",
                  },
                  system_message: {
                    trace_as_input: true,
                    trace_as_metadata: true,
                    load_from_db: false,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "system_message",
                    value: "",
                    display_name: "System Message",
                    advanced: true,
                    input_types: ["Message"],
                    dynamic: false,
                    info: "System message to pass to the model.",
                    title_case: false,
                    type: "str",
                    _input_type: "MessageTextInput",
                  },
                  temperature: {
                    trace_as_metadata: true,
                    list: false,
                    required: false,
                    placeholder: "",
                    show: true,
                    name: "temperature",
                    value: 0.1,
                    display_name: "Temperature",
                    advanced: false,
                    dynamic: false,
                    info: "Run inference with this temperature. Must by in the closed interval [0.0, 1.0].",
                    title_case: false,
                    type: "float",
                    _input_type: "FloatInput",
                  },
                },
                description: "Generate text using Groq.",
                icon: "Groq",
                base_classes: ["LanguageModel", "Message"],
                display_name: "Groq",
                documentation: "",
                custom_fields: {},
                output_types: [],
                pinned: false,
                conditional_paths: [],
                frozen: false,
                outputs: [
                  {
                    types: ["Message"],
                    selected: "Message",
                    name: "text_output",
                    display_name: "Text",
                    method: "text_response",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: [
                      "input_value",
                      "stream",
                      "system_message",
                    ],
                  },
                  {
                    types: ["LanguageModel"],
                    selected: "LanguageModel",
                    name: "model_output",
                    display_name: "Language Model",
                    method: "build_model",
                    value: "__UNDEFINED__",
                    cache: true,
                    required_inputs: [
                      "groq_api_base",
                      "groq_api_key",
                      "max_tokens",
                      "model_name",
                      "n",
                      "stream",
                      "temperature",
                    ],
                  },
                ],
                field_order: [
                  "input_value",
                  "system_message",
                  "stream",
                  "groq_api_key",
                  "groq_api_base",
                  "max_tokens",
                  "temperature",
                  "n",
                  "model_name",
                  "output_parser",
                ],
                beta: false,
                edited: false,
                metadata: {},
                lf_version: "1.0.19.post1",
              },
              id: "GroqModel-HCKnM",
              description: "Generate text using Groq.",
              display_name: "Groq",
            },
            selected: false,
            width: 384,
            height: 601,
            positionAbsolute: { x: 3404.128301641932, y: 188.7753644352846 },
            dragging: false,
          },
        ],
        edges: [
          {
            source: "ChatInput-6GHDU",
            target: "Prompt-1Fok7",
            sourceHandle:
              "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-6GHDUœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
            targetHandle:
              "{œfieldNameœ:œquestionœ,œidœ:œPrompt-1Fok7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
            id: "reactflow__edge-ChatInput-6GHDU{œdataTypeœ:œChatInputœ,œidœ:œChatInput-6GHDUœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-1Fok7{œfieldNameœ:œquestionœ,œidœ:œPrompt-1Fok7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
            data: {
              targetHandle: {
                fieldName: "question",
                id: "Prompt-1Fok7",
                inputTypes: ["Message", "Text"],
                type: "str",
              },
              sourceHandle: {
                dataType: "ChatInput",
                id: "ChatInput-6GHDU",
                name: "message",
                output_types: ["Message"],
              },
            },
            selected: false,
            className: "",
            animated: false,
          },
          {
            source: "File-e5qwb",
            target: "SplitText-0B9NJ",
            sourceHandle:
              "{œdataTypeœ:œFileœ,œidœ:œFile-e5qwbœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}",
            targetHandle:
              "{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-0B9NJœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
            id: "reactflow__edge-File-e5qwb{œdataTypeœ:œFileœ,œidœ:œFile-e5qwbœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-SplitText-0B9NJ{œfieldNameœ:œdata_inputsœ,œidœ:œSplitText-0B9NJœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
            data: {
              targetHandle: {
                fieldName: "data_inputs",
                id: "SplitText-0B9NJ",
                inputTypes: ["Data"],
                type: "other",
              },
              sourceHandle: {
                dataType: "File",
                id: "File-e5qwb",
                name: "data",
                output_types: ["Data"],
              },
            },
            selected: false,
            className: "",
            animated: false,
          },
          {
            source: "ChatInput-6GHDU",
            target: "Chroma-dOmCo",
            sourceHandle:
              "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-6GHDUœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
            targetHandle:
              "{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-dOmCoœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            id: "reactflow__edge-ChatInput-6GHDU{œdataTypeœ:œChatInputœ,œidœ:œChatInput-6GHDUœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Chroma-dOmCo{œfieldNameœ:œsearch_queryœ,œidœ:œChroma-dOmCoœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            data: {
              targetHandle: {
                fieldName: "search_query",
                id: "Chroma-dOmCo",
                inputTypes: ["Message"],
                type: "str",
              },
              sourceHandle: {
                dataType: "ChatInput",
                id: "ChatInput-6GHDU",
                name: "message",
                output_types: ["Message"],
              },
            },
            selected: false,
            className: "",
            animated: false,
          },
          {
            source: "Chroma-dOmCo",
            target: "ParseData-GuEEQ",
            sourceHandle:
              "{œdataTypeœ:œChromaœ,œidœ:œChroma-dOmCoœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
            targetHandle:
              "{œfieldNameœ:œdataœ,œidœ:œParseData-GuEEQœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
            id: "reactflow__edge-Chroma-dOmCo{œdataTypeœ:œChromaœ,œidœ:œChroma-dOmCoœ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ParseData-GuEEQ{œfieldNameœ:œdataœ,œidœ:œParseData-GuEEQœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
            data: {
              targetHandle: {
                fieldName: "data",
                id: "ParseData-GuEEQ",
                inputTypes: ["Data"],
                type: "other",
              },
              sourceHandle: {
                dataType: "Chroma",
                id: "Chroma-dOmCo",
                name: "search_results",
                output_types: ["Data"],
              },
            },
            selected: false,
            className: "",
            animated: false,
          },
          {
            source: "TextInput-eczLx",
            target: "Memory-aCsXk",
            sourceHandle:
              "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-eczLxœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
            targetHandle:
              "{œfieldNameœ:œsession_idœ,œidœ:œMemory-aCsXkœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            id: "reactflow__edge-TextInput-eczLx{œdataTypeœ:œTextInputœ,œidœ:œTextInput-eczLxœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Memory-aCsXk{œfieldNameœ:œsession_idœ,œidœ:œMemory-aCsXkœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            data: {
              targetHandle: {
                fieldName: "session_id",
                id: "Memory-aCsXk",
                inputTypes: ["Message"],
                type: "str",
              },
              sourceHandle: {
                dataType: "TextInput",
                id: "TextInput-eczLx",
                name: "text",
                output_types: ["Message"],
              },
            },
            selected: false,
            className: "",
            animated: false,
          },
          {
            source: "Memory-aCsXk",
            target: "Prompt-1Fok7",
            sourceHandle:
              "{œdataTypeœ:œMemoryœ,œidœ:œMemory-aCsXkœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}",
            targetHandle:
              "{œfieldNameœ:œhistoryœ,œidœ:œPrompt-1Fok7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
            id: "reactflow__edge-Memory-aCsXk{œdataTypeœ:œMemoryœ,œidœ:œMemory-aCsXkœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-1Fok7{œfieldNameœ:œhistoryœ,œidœ:œPrompt-1Fok7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
            data: {
              targetHandle: {
                fieldName: "history",
                id: "Prompt-1Fok7",
                inputTypes: ["Message", "Text"],
                type: "str",
              },
              sourceHandle: {
                dataType: "Memory",
                id: "Memory-aCsXk",
                name: "messages_text",
                output_types: ["Message"],
              },
            },
            selected: false,
            className: "",
            animated: false,
          },
          {
            source: "TextInput-eczLx",
            target: "ChatInput-6GHDU",
            sourceHandle:
              "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-eczLxœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
            targetHandle:
              "{œfieldNameœ:œsession_idœ,œidœ:œChatInput-6GHDUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            id: "reactflow__edge-TextInput-eczLx{œdataTypeœ:œTextInputœ,œidœ:œTextInput-eczLxœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ChatInput-6GHDU{œfieldNameœ:œsession_idœ,œidœ:œChatInput-6GHDUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            data: {
              targetHandle: {
                fieldName: "session_id",
                id: "ChatInput-6GHDU",
                inputTypes: ["Message"],
                type: "str",
              },
              sourceHandle: {
                dataType: "TextInput",
                id: "TextInput-eczLx",
                name: "text",
                output_types: ["Message"],
              },
            },
            selected: false,
            className: "",
            animated: false,
          },
          {
            source: "MistalAIEmbeddings-KEi53",
            sourceHandle:
              "{œdataTypeœ:œMistalAIEmbeddingsœ,œidœ:œMistalAIEmbeddings-KEi53œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
            target: "Chroma-dOmCo",
            targetHandle:
              "{œfieldNameœ:œembeddingœ,œidœ:œChroma-dOmCoœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
            data: {
              targetHandle: {
                fieldName: "embedding",
                id: "Chroma-dOmCo",
                inputTypes: ["Embeddings"],
                type: "other",
              },
              sourceHandle: {
                dataType: "MistalAIEmbeddings",
                id: "MistalAIEmbeddings-KEi53",
                name: "embeddings",
                output_types: ["Embeddings"],
              },
            },
            id: "reactflow__edge-MistalAIEmbeddings-KEi53{œdataTypeœ:œMistalAIEmbeddingsœ,œidœ:œMistalAIEmbeddings-KEi53œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Chroma-dOmCo{œfieldNameœ:œembeddingœ,œidœ:œChroma-dOmCoœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
            className: "",
            animated: false,
          },
          {
            source: "SplitText-0B9NJ",
            sourceHandle:
              "{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-0B9NJœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}",
            target: "Chroma-gXf1F",
            targetHandle:
              "{œfieldNameœ:œingest_dataœ,œidœ:œChroma-gXf1Fœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
            data: {
              targetHandle: {
                fieldName: "ingest_data",
                id: "Chroma-gXf1F",
                inputTypes: ["Data"],
                type: "other",
              },
              sourceHandle: {
                dataType: "SplitText",
                id: "SplitText-0B9NJ",
                name: "chunks",
                output_types: ["Data"],
              },
            },
            id: "reactflow__edge-SplitText-0B9NJ{œdataTypeœ:œSplitTextœ,œidœ:œSplitText-0B9NJœ,œnameœ:œchunksœ,œoutput_typesœ:[œDataœ]}-Chroma-gXf1F{œfieldNameœ:œingest_dataœ,œidœ:œChroma-gXf1Fœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
            className: "",
            animated: false,
          },
          {
            source: "MistalAIEmbeddings-KEi53",
            sourceHandle:
              "{œdataTypeœ:œMistalAIEmbeddingsœ,œidœ:œMistalAIEmbeddings-KEi53œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
            target: "Chroma-gXf1F",
            targetHandle:
              "{œfieldNameœ:œembeddingœ,œidœ:œChroma-gXf1Fœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
            data: {
              targetHandle: {
                fieldName: "embedding",
                id: "Chroma-gXf1F",
                inputTypes: ["Embeddings"],
                type: "other",
              },
              sourceHandle: {
                dataType: "MistalAIEmbeddings",
                id: "MistalAIEmbeddings-KEi53",
                name: "embeddings",
                output_types: ["Embeddings"],
              },
            },
            id: "reactflow__edge-MistalAIEmbeddings-KEi53{œdataTypeœ:œMistalAIEmbeddingsœ,œidœ:œMistalAIEmbeddings-KEi53œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-Chroma-gXf1F{œfieldNameœ:œembeddingœ,œidœ:œChroma-gXf1Fœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
            className: "",
            animated: false,
          },
          {
            source: "ParseData-GuEEQ",
            sourceHandle:
              "{œdataTypeœ:œParseDataœ,œidœ:œParseData-GuEEQœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
            target: "Prompt-1Fok7",
            targetHandle:
              "{œfieldNameœ:œcontextœ,œidœ:œPrompt-1Fok7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
            data: {
              targetHandle: {
                fieldName: "context",
                id: "Prompt-1Fok7",
                inputTypes: ["Message", "Text"],
                type: "str",
              },
              sourceHandle: {
                dataType: "ParseData",
                id: "ParseData-GuEEQ",
                name: "text",
                output_types: ["Message"],
              },
            },
            id: "reactflow__edge-ParseData-GuEEQ{œdataTypeœ:œParseDataœ,œidœ:œParseData-GuEEQœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-1Fok7{œfieldNameœ:œcontextœ,œidœ:œPrompt-1Fok7œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
            className: "",
            animated: false,
          },
          {
            source: "GroqModel-HCKnM",
            sourceHandle:
              "{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-HCKnMœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
            target: "ChatOutput-4D7QB",
            targetHandle:
              "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-4D7QBœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            data: {
              targetHandle: {
                fieldName: "input_value",
                id: "ChatOutput-4D7QB",
                inputTypes: ["Message"],
                type: "str",
              },
              sourceHandle: {
                dataType: "GroqModel",
                id: "GroqModel-HCKnM",
                name: "text_output",
                output_types: ["Message"],
              },
            },
            id: "reactflow__edge-GroqModel-HCKnM{œdataTypeœ:œGroqModelœ,œidœ:œGroqModel-HCKnMœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-4D7QB{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-4D7QBœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            className: "",
            animated: false,
          },
          {
            source: "Prompt-1Fok7",
            sourceHandle:
              "{œdataTypeœ:œPromptœ,œidœ:œPrompt-1Fok7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
            target: "GroqModel-HCKnM",
            targetHandle:
              "{œfieldNameœ:œinput_valueœ,œidœ:œGroqModel-HCKnMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            data: {
              targetHandle: {
                fieldName: "input_value",
                id: "GroqModel-HCKnM",
                inputTypes: ["Message"],
                type: "str",
              },
              sourceHandle: {
                dataType: "Prompt",
                id: "Prompt-1Fok7",
                name: "prompt",
                output_types: ["Message"],
              },
            },
            id: "reactflow__edge-Prompt-1Fok7{œdataTypeœ:œPromptœ,œidœ:œPrompt-1Fok7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GroqModel-HCKnM{œfieldNameœ:œinput_valueœ,œidœ:œGroqModel-HCKnMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
            className: "",
            animated: false,
          },
        ],
        viewport: {
          x: 158.7239372737116,
          y: 74.01594544351258,
          zoom: 0.214606664253815,
        },
      },
      description: "Empowering Enterprises with Intelligent Interactions.",
      name: "rag chatbot (1)",
      last_tested_version: "1.0.19.post1",
      endpoint_name: "LeinadFlow-1",
      is_component: false,
    });
  } catch (error) {
    console.error(error);
    return NextResponse.json(
      { message: "Error returning data" },
      { status: 500 }
    );
  }
}
